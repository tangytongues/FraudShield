menu

[Skip to\\
\\
content](https://www.kaggle.com/code/benroshan/transaction-fraud-detection/notebook#site-content)

[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)

Create

search​

- [explore\\
\\
Home](https://www.kaggle.com/)

- [emoji\_events\\
\\
Competitions](https://www.kaggle.com/competitions)

- [table\_chart\\
\\
Datasets](https://www.kaggle.com/datasets)

- [tenancy\\
\\
Models](https://www.kaggle.com/models)

- [code\\
\\
Code](https://www.kaggle.com/code)

- [comment\\
\\
Discussions](https://www.kaggle.com/discussions)

- [school\\
\\
Learn](https://www.kaggle.com/learn)


- [expand\_more\\
\\
More](https://www.kaggle.com/code/benroshan/transaction-fraud-detection/notebook#)


auto\_awesome\_motion

View Active Events

menu

[Skip to\\
\\
content](https://www.kaggle.com/code/benroshan/transaction-fraud-detection/notebook#site-content)

[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)

search​

[Sign In](https://www.kaggle.com/account/login?phase=startSignInTab&returnUrl=%2Fcode%2Fbenroshan%2Ftransaction-fraud-detection%2Fnotebook)

[Register](https://www.kaggle.com/account/login?phase=startRegisterTab&returnUrl=%2Fcode%2Fbenroshan%2Ftransaction-fraud-detection%2Fnotebook)

Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.

[Learn more](https://www.kaggle.com/cookies)

OK, Got it.

[Ben Roshan 's profile](https://www.kaggle.com/benroshan) Ben Roshan  · 4y ago · 25,289 views

arrow\_drop\_up67

Copy & Edit
422

![bronze medal](https://www.kaggle.com/static/images/medals/notebooks/bronzel@1x.png)

more\_vert

# 💵Transaction Fraud Detection🕵️‍♂️

## 💵Transaction Fraud Detection🕵️‍♂️

[Notebook](https://www.kaggle.com/code/benroshan/transaction-fraud-detection/notebook) [Input](https://www.kaggle.com/code/benroshan/transaction-fraud-detection/input) [Output](https://www.kaggle.com/code/benroshan/transaction-fraud-detection/output) [Logs](https://www.kaggle.com/code/benroshan/transaction-fraud-detection/log) [Comments (1)](https://www.kaggle.com/code/benroshan/transaction-fraud-detection/comments)

historyVersion 2 of 2chevron\_right

## Runtime

play\_arrow

5s

## Input

DATASETS

![](https://storage.googleapis.com/kaggle-datasets-images/1069/1927/4cbcc70173f19dc478109f129adc91a8/dataset-thumbnail.jpg)

paysim1

## Tags

[Business](https://www.kaggle.com/code?tagIds=11102-Business) [Finance](https://www.kaggle.com/code?tagIds=11108-Finance) [Banking](https://www.kaggle.com/code?tagIds=11129-Banking) [Feature Engineering](https://www.kaggle.com/code?tagIds=13306-Feature+Engineering)

## Language

Python

## Table of Contents

[✨Introduction](https://www.kaggle.com/code/benroshan/transaction-fraud-detection/notebook#%E2%9C%A8Introduction) [🤝 Libraries](https://www.kaggle.com/code/benroshan/transaction-fraud-detection/notebook#%F0%9F%A4%9D-Libraries) [🏧 Dataset](https://www.kaggle.com/code/benroshan/transaction-fraud-detection/notebook#%F0%9F%8F%A7-Dataset) [📋 Pivot table analysis](https://www.kaggle.com/code/benroshan/transaction-fraud-detection/notebook#%F0%9F%93%8B-Pivot-table-analysis) [📊 Distribution of Amount](https://www.kaggle.com/code/benroshan/transaction-fraud-detection/notebook#%F0%9F%93%8A-Distribution-of-Amount) [🔧 Feature engineering](https://www.kaggle.com/code/benroshan/transaction-fraud-detection/notebook#%F0%9F%94%A7-Feature-engineering) [⚙️ Pre-processing data](https://www.kaggle.com/code/benroshan/transaction-fraud-detection/notebook#%E2%9A%99%EF%B8%8F-Pre-processing-data) [1\. Balancing the target](https://www.kaggle.com/code/benroshan/transaction-fraud-detection/notebook#1.-Balancing-the-target) [2\. One hot encoding](https://www.kaggle.com/code/benroshan/transaction-fraud-detection/notebook#2.-One-hot-encoding) [3\. Split and Standardize](https://www.kaggle.com/code/benroshan/transaction-fraud-detection/notebook#3.-Split-and-Standardize) [3\. Tokenization](https://www.kaggle.com/code/benroshan/transaction-fraud-detection/notebook#3.-Tokenization) [🤖 Model Building](https://www.kaggle.com/code/benroshan/transaction-fraud-detection/notebook#%F0%9F%A4%96-Model-Building) [⛮ Hyperparameter Tuning](https://www.kaggle.com/code/benroshan/transaction-fraud-detection/notebook#%E2%9B%AE-Hyperparameter-Tuning) [🧪 Evaluation of model](https://www.kaggle.com/code/benroshan/transaction-fraud-detection/notebook#%F0%9F%A7%AA-Evaluation-of-model) [🍃 Conclusion](https://www.kaggle.com/code/benroshan/transaction-fraud-detection/notebook#%F0%9F%8D%83-Conclusion)

\_\_notebook\_\_

Loading \[MathJax\]/extensions/Safe.js

linkcode

# ✨Introduction [¶](https://www.kaggle.com/code/benroshan/transaction-fraud-detection/notebook\#%E2%9C%A8Introduction)

![](https://m6r6k8y2.rocketcdn.me/wp-content/uploads/2020/12/cyber-theft-senior-fraud-GIF.gif)

> 🗿 **History:** Ever since the advent of internet the digital revolution has rising and has creeped into all aspects to our lives. One of the most important digital revolution happend in financial system and especially transacting money to someone from any part of the world digitally. Digital transactions have become a part of daily life like purchasing a product online, sending money to friends, depositing cash in bank account, investment purposes etc., They had a lot of benefits so does paved way for fradulent activities. People started using digital money transactions medium to launder money and make the money look like it comes from a legal source.
>
> 🎯 **Objective:** The objective of this notebook is to find the patterns of transactions performed and help algorithms learn those patterns in identifying the fradulent transactions and flag them
>
> 📌 **Goals:**
>
> 1. Exploratory analysis of data to extract the pattern of fraudlent activites
> 2. Build a machine learning model to classify fraud and non-fraud transactions
> 3. Reduce the false negatives by tuning the model

## 🤝 Libraries [¶](https://www.kaggle.com/code/benroshan/transaction-fraud-detection/notebook\#%F0%9F%A4%9D-Libraries)

In \[47\]:

linkcode

```
#Basic libraries
import pandas as pd
import numpy as np

#Visualization libraries
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
from plotly.offline import plot, iplot, init_notebook_mode
init_notebook_mode(connected=True)
%matplotlib inline

#preprocessing libraries
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

#ML libraries
import tensorflow as tf
from sklearn.svm import SVC
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.svm import LinearSVC
from sklearn.pipeline import Pipeline

#Metrics Libraries
from sklearn import metrics
from sklearn.metrics import classification_report
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix

#Misc libraries
import warnings
warnings.filterwarnings("ignore")

```

linkcode

# 🏧 Dataset [¶](https://www.kaggle.com/code/benroshan/transaction-fraud-detection/notebook\#%F0%9F%8F%A7-Dataset)

> Here we just import the first 50000 rows since the dataset is huge and would take a long time to process. Let's see the overview of data

In \[48\]:

linkcode

```
#Reading the data
paysim=pd.read_csv('../input/paysim1/PS_20174392719_1491204439457_log.csv')

#Looking at the data
paysim.head()

```

Out\[48\]:

|  | step | type | amount | nameOrig | oldbalanceOrg | newbalanceOrig | nameDest | oldbalanceDest | newbalanceDest | isFraud | isFlaggedFraud |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 0 | 1 | PAYMENT | 9839.64 | C1231006815 | 170136.0 | 160296.36 | M1979787155 | 0.0 | 0.0 | 0 | 0 |
| 1 | 1 | PAYMENT | 1864.28 | C1666544295 | 21249.0 | 19384.72 | M2044282225 | 0.0 | 0.0 | 0 | 0 |
| 2 | 1 | TRANSFER | 181.00 | C1305486145 | 181.0 | 0.00 | C553264065 | 0.0 | 0.0 | 1 | 0 |
| 3 | 1 | CASH\_OUT | 181.00 | C840083671 | 181.0 | 0.00 | C38997010 | 21182.0 | 0.0 | 1 | 0 |
| 4 | 1 | PAYMENT | 11668.14 | C2048537720 | 41554.0 | 29885.86 | M1230701703 | 0.0 | 0.0 | 0 | 0 |

In \[49\]:

linkcode

```
paysim.info()

```

```
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 6362620 entries, 0 to 6362619
Data columns (total 11 columns):
 #   Column          Dtype
---  ------          -----
 0   step            int64
 1   type            object
 2   amount          float64
 3   nameOrig        object
 4   oldbalanceOrg   float64
 5   newbalanceOrig  float64
 6   nameDest        object
 7   oldbalanceDest  float64
 8   newbalanceDest  float64
 9   isFraud         int64
 10  isFlaggedFraud  int64
dtypes: float64(5), int64(3), object(3)
memory usage: 534.0+ MB

```

linkcode

### Dataset description [¶](https://www.kaggle.com/code/benroshan/transaction-fraud-detection/notebook\#Dataset-description)

> 01. step - maps a unit of time in the real world. In this case 1 step is 1 hour of time. Total steps 744 (30 days simulation).
> 02. type - CASH-IN, CASH-OUT, DEBIT, PAYMENT and TRANSFER.
> 03. amount - amount of the transaction in local currency.
> 04. nameOrig - customer who started the transaction
> 05. oldbalanceOrg - initial balance before the transaction
> 06. newbalanceOrig - new balance after the transaction
> 07. nameDest - customer who is the recipient of the transaction
> 08. oldbalanceDest - initial balance recipient before the transaction. Note that there is not information for customers that start with M (Merchants).
> 09. newbalanceDest - new balance recipient after the transaction. Note that there is not information for customers that start with M (Merchants).
> 10. isFraud - This is the transactions made by the fraudulent agents inside the simulation. In this specific dataset the fraudulent behavior of the agents aims to profit by taking control or customers accounts and try to empty the funds by transferring to another account and then cashing out of the system.
> 11. isFlaggedFraud - The business model aims to control massive transfers from one account to another and flags illegal attempts. An illegal attempt in this dataset is an attempt to transfer more than 200.000 in a single transaction.

linkcode

# 📋 Pivot table analysis [¶](https://www.kaggle.com/code/benroshan/transaction-fraud-detection/notebook\#%F0%9F%93%8B-Pivot-table-analysis)

> Numbers is everything in transaction monitoring. Numbers decide whether it is a fraudulent activity or not. Let us look at the overall numbers using pivot function

In \[50\]:

linkcode

```
#Pivot table
paysim_pivot1=pd.pivot_table(paysim,index=["type"],
                               values=['amount','isFraud','isFlaggedFraud'],
                               aggfunc=[np.sum,np.std], margins=True)

#Adding color gradient
cm = sns.light_palette("green", as_cmap=True)
paysim_pivot1.style.background_gradient(cmap=cm)

```

Out\[50\]:

|  | sum | std |
| --- | --- | --- |
|  | amount | isFlaggedFraud | isFraud | amount | isFlaggedFraud | isFraud |
| --- | --- | --- | --- | --- | --- | --- |
| type |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- |
| CASH\_IN | 236367391912.458771 | 0 | 0 | 126508.255272 | 0.000000 | 0.000000 |
| CASH\_OUT | 394412995224.487915 | 0 | 4116 | 175329.744483 | 0.000000 | 0.042851 |
| DEBIT | 227199221.279999 | 0 | 0 | 13318.535518 | 0.000000 | 0.000000 |
| PAYMENT | 28093371138.369801 | 0 | 0 | 12556.450186 | 0.000000 | 0.000000 |
| TRANSFER | 485291987263.161987 | 16 | 4097 | 1879573.528908 | 0.005479 | 0.087344 |
| All | 1144392944759.770020 | 16 | 8213 | 603858.184009 | 0.001586 | 0.035905 |

linkcode

**🔍Inference:**

As per the current rule based algorithm, there has been no flags during fraud transactions incase of cash\_out , which a serious concern to the anti money laundering system . Also there are only 16 transactions which are flagged as fraud whereas around 4k transactions are actually fraud. Our mission is now to build an efficient algorithm to mitigate this risk of letting fraud transactions unblocked

In \[51\]:

linkcode

```
#Pivot table
paysim_pivot2=pd.pivot_table(paysim,index=["type"],
                               values=['amount','oldbalanceOrg','newbalanceOrig'],
                               aggfunc=[np.sum], margins=True)

#Adding style
paysim_pivot2.style\
    .format('{:.2f}')\
    .bar(align='mid',color=['darkred'])\
    .set_properties(padding='5px',border='3px solid white',width='200px')

```

Out\[51\]:

|  | sum |
| --- | --- |
|  | amount | newbalanceOrig | oldbalanceOrg |
| --- | --- | --- | --- |
| type |  |  |  |
| --- | --- | --- | --- |
| CASH\_IN | 236367391912.46 | 5260438481752.40 | 5024078139747.30 |
| CASH\_OUT | 394412995224.49 | 39098506249.34 | 102978263227.81 |
| DEBIT | 227199221.28 | 2699777564.12 | 2844196471.80 |
| PAYMENT | 28093371138.37 | 133043913105.10 | 146768163438.79 |
| TRANSFER | 485291987263.16 | 5482651300.55 | 29012552760.76 |
| All | 1144392944759.77 | 5440763329971.49 | 5305681315646.41 |

linkcode

**🔍Inference:**

From the table we can understand that most of the customers use the system for transfering money and we have a relatively less data for payments made. Also it is quite interesting to notice the difference between the new and old balance as it tells us some stories. Here we have only the visuals of Orgin account and cash balance have reduced in all cases except cash\_in . Even in transfer the balance have reduced which shows that we have more sender information in Original account

In \[52\]:

linkcode

```
#Pivot table
paysim_pivot3=pd.pivot_table(paysim,index=["type"],
                               values=['amount','oldbalanceDest','newbalanceDest'],
                               aggfunc=[np.sum], margins=True)

#Adding style
paysim_pivot3.style\
    .format('{:.2f}')\
    .bar(align='mid',color=['darkblue'])\
    .set_properties(padding='5px',border='3px solid white',width='200px')

```

Out\[52\]:

|  | sum |
| --- | --- |
|  | amount | newbalanceDest | oldbalanceDest |
| --- | --- | --- | --- |
| type |  |  |  |
| --- | --- | --- | --- |
| CASH\_IN | 236367391912.46 | 2052897091879.15 | 2221949365238.98 |
| CASH\_OUT | 394412995224.49 | 3784342078061.15 | 3351233273577.78 |
| DEBIT | 227199221.28 | 62686759687.09 | 61863601275.22 |
| PAYMENT | 28093371138.37 | 0.00 | 0.00 |
| TRANSFER | 485291987263.16 | 1894260653500.26 | 1368300197339.44 |
| All | 1144392944759.77 | 7794186583127.56 | 7003346437431.25 |

linkcode

**🔍Inference:**

In this table we have the information of destination account , from the transfer information we can see the increase in new balance, hence this is the receiver's info. There is no payment amount available for Dest information.

linkcode

## 📊 Distribution of Amount [¶](https://www.kaggle.com/code/benroshan/transaction-fraud-detection/notebook\#%F0%9F%93%8A-Distribution-of-Amount)

> It is important to understand the distribution of our data, since it can play a major role in model building and also in understanding our data. Going forward we will be using only 50k rows as it takes a lot of time to process all the records for viz and model building.
> Here we check for the distribution of amount transacted using the application

In \[53\]:

linkcode

```
#Reading the first 50000 rows
paysim=pd.read_csv('../input/paysim1/PS_20174392719_1491204439457_log.csv',nrows=50000)

#Distribution of Amount
fig = px.box(paysim, y="amount")
fig.show()

```

02M4M6M8M10M

amount

[plotly-logomark](https://plotly.com/)

linkcode

**🔍Inference:**

From the bar plot we can understand that we have a very right skewed dataset, there are a lot of outliers which goes upto 10M with a median of 33k. The upper bracket(75th percentile) counts upto 450k

linkcode

# 🔧 Feature engineering [¶](https://www.kaggle.com/code/benroshan/transaction-fraud-detection/notebook\#%F0%9F%94%A7-Feature-engineering)

> Time to get our hands dirty with feature engineering. With the available information it is hard to train the model and get better results. Hence we move onto create new features by altering the existing features. In this we create three functions which creates a highly relevant feature for the domain
>
> 1. **Difference in balance:** It is an universal truth that the amount debited from senders account gets credited into the receivers account without any deviation in cents. But what if there is a deviation incase of the amount debited and credited. Some could be due to the charges levied by the service providers, yet we need to flag such unusual instances
> 2. **Surge indicator:** Also we have to trigger flag when large amount are involved in the transaction. From the distribution of amount we understood that we have a lot of outliers with high amount in transactions. Hence we consider the 75th percentile(450k) as our threshold and amount which is greater than 450k will be triggered as a flag
> 3. **Frequency indicator:** Here we flag the user and not the transaction. When there is a receiver who receives money from a lot of people, it could be a trigger as it can be for some illegal games of chance or luck. Hence it is flagged when there is a receiver who receives money for more than 20 times.
> 4. **Merchant indicator:** The customer ids in receiver starts with 'M' which means that they are merchants and they obviously will have a lot of receiving transactions. So we also flag whenever there is a merchant receiver

In \[54\]:

linkcode

```
#Tallying the balance
def balance_diff(data):
    '''balance_diff checks whether the money debited from sender has exactly credited to the receiver
       then it creates a new column which indicates 1 when there is a deviation else 0'''
    #Sender's balance
    orig_change=data['newbalanceOrig']-data['oldbalanceOrg']
    orig_change=orig_change.astype(int)
    for i in orig_change:
        if i<0:
            data['orig_txn_diff']=round(data['amount']+orig_change,2)
        else:
            data['orig_txn_diff']=round(data['amount']-orig_change,2)
    data['orig_txn_diff']=data['orig_txn_diff'].astype(int)
    data['orig_diff'] = [1 if n !=0 else 0 for n in data['orig_txn_diff']]

    #Receiver's balance
    dest_change=data['newbalanceDest']-data['oldbalanceDest']
    dest_change=dest_change.astype(int)
    for i in dest_change:
        if i<0:
            data['dest_txn_diff']=round(data['amount']+dest_change,2)
        else:
            data['dest_txn_diff']=round(data['amount']-dest_change,2)
    data['dest_txn_diff']=data['dest_txn_diff'].astype(int)
    data['dest_diff'] = [1 if n !=0 else 0 for n in data['dest_txn_diff']]

    data.drop(['orig_txn_diff','dest_txn_diff'],axis=1,inplace = True)

#Surge indicator
def surge_indicator(data):
    '''Creates a new column which has 1 if the transaction amount is greater than the threshold
    else it will be 0'''
    data['surge']=[1 if n>450000 else 0 for n in data['amount']]

#Frequency indicator
def frequency_receiver(data):
    '''Creates a new column which has 1 if the receiver receives money from many individuals
    else it will be 0'''
    data['freq_Dest']=data['nameDest'].map(data['nameDest'].value_counts())
    data['freq_dest']=[1 if n>20 else 0 for n in data['freq_Dest']]

    data.drop(['freq_Dest'],axis=1,inplace = True)

#Tracking the receiver as merchant or not
def merchant(data):
    '''We also have customer ids which starts with M in Receiver name, it indicates merchant
    this function will flag if there is a merchant in receiver end '''
    values = ['M']
    conditions = list(map(data['nameDest'].str.contains, values))
    data['merchant'] = np.select(conditions, '1', '0')

```

In \[55\]:

linkcode

```
#Applying balance_diff function
balance_diff(paysim)

paysim['orig_diff'].value_counts()
paysim['dest_diff'].value_counts()

```

Out\[55\]:

```
1    44994
0     5006
Name: dest_diff, dtype: int64
```

In \[56\]:

linkcode

```
#Applying surge_indicator function
surge_indicator(paysim)
paysim['surge'].value_counts()

```

Out\[56\]:

```
0    46392
1     3608
Name: surge, dtype: int64
```

In \[57\]:

linkcode

```
#Applying frequency_receiver function
frequency_receiver(paysim)
paysim['freq_dest'].value_counts()

```

Out\[57\]:

```
0    46295
1     3705
Name: freq_dest, dtype: int64
```

linkcode

# ⚙️ Pre-processing data [¶](https://www.kaggle.com/code/benroshan/transaction-fraud-detection/notebook\#%E2%9A%99%EF%B8%8F-Pre-processing-data)

> Before moving to build a machine learning model, it is mandatory to pre-process the data so that the model trains without any error and can learn better to provide better results

## 1\. Balancing the target [¶](https://www.kaggle.com/code/benroshan/transaction-fraud-detection/notebook\#1.-Balancing-the-target)

> From the pie chart below we can clearly see that the target label is heavily imbalance as we have only 0.2% of fraudulent data which is in-sufficient for machine to learn and flag when fraud transactions happen.

In \[58\]:

linkcode

```
#Creating a copy
paysim_1=paysim.copy()

#Checking for balance in target
fig = go.Figure(data=[go.Pie(labels=['Not Fraud','Fraud'], values=paysim_1['isFraud'].value_counts())])
fig.show()

```

99.8%0.2%

Not FraudFraud

[plotly-logomark](https://plotly.com/)

In \[59\]:

linkcode

```
#Getting the max size
max_size = paysim_1['isFraud'].value_counts().max()

#Balancing the target label
lst = [paysim_1]
for class_index, group in paysim_1.groupby('isFraud'):
    lst.append(group.sample(max_size-len(group), replace=True))
paysim_1 = pd.concat(lst)

```

In \[60\]:

linkcode

```
#Checking the balanced target
fig = go.Figure(data=[go.Pie(labels=['Not Fraud','Fraud'], values=paysim_1['isFraud'].value_counts())])
fig.show()

```

50%50%

Not FraudFraud

[plotly-logomark](https://plotly.com/)

linkcode

## 2\. One hot encoding [¶](https://www.kaggle.com/code/benroshan/transaction-fraud-detection/notebook\#2.-One-hot-encoding)

> One of the most important feature we have is type which is categorical in type. Since it doesnt have any ordinal nature and since the classes are less, we prefer applying one hot encoding.

In \[61\]:

linkcode

```
#One hot encoding
paysim_1=pd.concat([paysim_1,pd.get_dummies(paysim_1['type'], prefix='type_')],axis=1)
paysim_1.drop(['type'],axis=1,inplace = True)

paysim_1.head()

```

Out\[61\]:

|  | step | amount | nameOrig | oldbalanceOrg | newbalanceOrig | nameDest | oldbalanceDest | newbalanceDest | isFraud | isFlaggedFraud | orig\_diff | dest\_diff | surge | freq\_dest | type\_\_CASH\_IN | type\_\_CASH\_OUT | type\_\_DEBIT | type\_\_PAYMENT | type\_\_TRANSFER |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 0 | 1 | 9839.64 | C1231006815 | 170136.0 | 160296.36 | M1979787155 | 0.0 | 0.0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 |
| 1 | 1 | 1864.28 | C1666544295 | 21249.0 | 19384.72 | M2044282225 | 0.0 | 0.0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 |
| 2 | 1 | 181.00 | C1305486145 | 181.0 | 0.00 | C553264065 | 0.0 | 0.0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 |
| 3 | 1 | 181.00 | C840083671 | 181.0 | 0.00 | C38997010 | 21182.0 | 0.0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 |
| 4 | 1 | 11668.14 | C2048537720 | 41554.0 | 29885.86 | M1230701703 | 0.0 | 0.0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 |

linkcode

## 3\. Split and Standardize [¶](https://www.kaggle.com/code/benroshan/transaction-fraud-detection/notebook\#3.-Split-and-Standardize)

> In this module we create the independent and dependent feature, then split them into train and test data where training size is 70%. Later we collect all the numerical features and apply StandardScaler() function which transforms the distribution so that the mean becomes 0 and standard deviation becomes 1

In \[62\]:

linkcode

```
#Splitting dependent and independent variable
paysim_2=paysim_1.copy()
X=paysim_2.drop('isFraud',axis=1)
y=paysim_2['isFraud']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=111)

#Standardizing the numerical columns
col_names=['amount','oldbalanceOrg','newbalanceOrig','oldbalanceDest','newbalanceDest']
features_train = X_train[col_names]
features_test = X_test[col_names]
scaler = StandardScaler().fit(features_train.values)
features_train = scaler.transform(features_train.values)
features_test = scaler.transform(features_test.values)
X_train[col_names] = features_train
X_test[col_names] =features_test

```

linkcode

## 3\. Tokenization [¶](https://www.kaggle.com/code/benroshan/transaction-fraud-detection/notebook\#3.-Tokenization)

> We had the customer ids and merchant ids stored in object type. It is bad to apply one hot encoding in it as it can lead to more features and curse of dimensionality can incur. Hence we are applying tokenization here as it can create an unique id number which is in 'int' type for each customer id

In \[63\]:

linkcode

```
#Tokenzation of customer name to get a unique id
tokenizer_org = tf.keras.preprocessing.text.Tokenizer()
tokenizer_org.fit_on_texts(X_train['nameOrig'])

tokenizer_dest = tf.keras.preprocessing.text.Tokenizer()
tokenizer_dest.fit_on_texts(X_train['nameDest'])

# Create tokenized customer lists
customers_train_org = tokenizer_org.texts_to_sequences(X_train['nameOrig'])
customers_test_org = tokenizer_org.texts_to_sequences(X_test['nameOrig'])

customers_train_dest = tokenizer_dest.texts_to_sequences(X_train['nameDest'])
customers_test_dest = tokenizer_dest.texts_to_sequences(X_test['nameDest'])

# Pad sequences
X_train['customers_org'] = tf.keras.preprocessing.sequence.pad_sequences(customers_train_org, maxlen=1)
X_test['customers_org'] = tf.keras.preprocessing.sequence.pad_sequences(customers_test_org, maxlen=1)

X_train['customers_dest'] = tf.keras.preprocessing.sequence.pad_sequences(customers_train_dest, maxlen=1)
X_test['customers_dest'] = tf.keras.preprocessing.sequence.pad_sequences(customers_test_dest, maxlen=1)

```

linkcode

### Dropping unnecessary columns [¶](https://www.kaggle.com/code/benroshan/transaction-fraud-detection/notebook\#Dropping-unnecessary-columns)

> We dont need the sender and receiver id as we have tokenized them, also we dont required isFlaggedFraud as it is just an outcome of current algorithm.

In \[64\]:

linkcode

```
#Dropping unnecessary columns
X_train=X_train.drop(['nameOrig','nameDest','isFlaggedFraud'],axis=1)
X_train = X_train.reset_index(drop=True)

X_test=X_test.drop(['nameOrig','nameDest','isFlaggedFraud'],axis=1)
X_test = X_test.reset_index(drop=True)

```

linkcode

# 🤖 Model Building [¶](https://www.kaggle.com/code/benroshan/transaction-fraud-detection/notebook\#%F0%9F%A4%96-Model-Building)

> We have successfully processed the data and it is time for serving the data to the model. It is time consuming to find out which model works best for our data. Hence I have utlized pipeline to run our data through all the classification algorithm and select the best which gives out the maximum accuracy.

In \[65\]:

linkcode

```
#creating the objects
logreg_cv = LogisticRegression(solver='liblinear',random_state=123)
dt_cv=DecisionTreeClassifier(random_state=123)
knn_cv=KNeighborsClassifier()
svc_cv=SVC(kernel='linear',random_state=123)
nb_cv=GaussianNB()
rf_cv=RandomForestClassifier(random_state=123)
cv_dict = {0: 'Logistic Regression', 1: 'Decision Tree',2:'KNN',3:'SVC',4:'Naive Bayes',5:'Random Forest'}
cv_models=[logreg_cv,dt_cv,knn_cv,svc_cv,nb_cv,rf_cv]

for i,model in enumerate(cv_models):
    print("{} Test Accuracy: {}".format(cv_dict[i],cross_val_score(model, X_train, y_train, cv=10, scoring ='accuracy').mean()))

```

```
Logistic Regression Test Accuracy: 0.9655167477812767
Decision Tree Test Accuracy: 0.9749498997995992
KNN Test Accuracy: 0.9877755511022045
SVC Test Accuracy: 0.9756369882622387
Naive Bayes Test Accuracy: 0.9946321213856283
Random Forest Test Accuracy: 0.9931291153736044

```

linkcode

**💭Thoughts:**

We can see who won the prize-it is Naive Bayes. Other algorithms have also performed in par with NB especially Random Forest and KNN. It sure looks overfitted as the accuracy is near 100% which can be verified using the test data. Before that lets do hyperparameter tuning on NB

linkcode

## ⛮ Hyperparameter Tuning [¶](https://www.kaggle.com/code/benroshan/transaction-fraud-detection/notebook\#%E2%9B%AE-Hyperparameter-Tuning)

> Lets fit the Naive bayes model by tuning the model with its parameters. Here we are gonna tune var\_smoothing which is a stability calculation to widen (or smooth) the curve and therefore account for more samples that are further away from the distribution mean. In this case, np.logspace returns numbers spaced evenly on a log scale, starts from 0, ends at -9, and generates 100 samples.

In \[66\]:

linkcode

```
param_grid_nb = {
    'var_smoothing': np.logspace(0,-9, num=100)
}

nbModel_grid = GridSearchCV(estimator=GaussianNB(), param_grid=param_grid_nb, verbose=1, cv=10, n_jobs=-1)
nbModel_grid.fit(X_train, y_train)
print(nbModel_grid.best_estimator_)

```

```
Fitting 10 folds for each of 100 candidates, totalling 1000 fits

```

```
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.
[Parallel(n_jobs=-1)]: Done  76 tasks      | elapsed:    1.6s
[Parallel(n_jobs=-1)]: Done 376 tasks      | elapsed:    7.5s
[Parallel(n_jobs=-1)]: Done 876 tasks      | elapsed:   17.6s

```

```
GaussianNB(var_smoothing=8.111308307896856e-09)

```

```
[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:   20.1s finished

```

linkcode

# 🧪 Evaluation of model [¶](https://www.kaggle.com/code/benroshan/transaction-fraud-detection/notebook\#%F0%9F%A7%AA-Evaluation-of-model)

> Time to explore the truth of high numbers by evaluating against testing data

unfold\_moreShow hidden code

In \[67\]:

linkcode

```
#Function for Confusion matrix
def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    thresh = cm.max() / 2.
    for i in range (cm.shape[0]):
        for j in range (cm.shape[1]):
            plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

```

In \[68\]:

linkcode

```
#Predict with the selected best parameter
y_pred=nbModel_grid.predict(X_test)

#Plotting confusion matrix
cm = metrics.confusion_matrix(y_test, y_pred)
plot_confusion_matrix(cm, classes=['Not Fraud','Fraud'])

```

```
Confusion matrix, without normalization

```

![](https://www.kaggleusercontent.com/kf/71667338/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..F9MZpwu0utKS0lIbrEOZVQ.F5r4NsHwuOoYWvo70-Pdx7JFQUXBWVBSzQKdgR02WOLozqlUsYGXfU7qoDAZ9SQxmfjP52O3pKJfXrkAMwK8-ALwlLiOVWfnPu3_Khg6O_GNlG8-NFKjyGt6JI4n5eS2DxFwEoJZq2cqonC77v08RN7-twKFnNbppbiax9ihdZhXm5m_SnrHK99N_BeJ6JU6sFBh10nwSzZTxJgTPwrZbWJQLgDz1ncB_Nd4pIsQGbJuNs2zamlW18tL6YCzPvKT-kMaTgFwCfWscZAqYRz0aw6kJYGIPvvrtCZwuK2vKhWL1IPIAbvcWT3KApE1ERIcBZeKyJy8a6p-8qqj2B7JHKpRqcYFZEtXFSxjEczZmkKV3JlWZCAzezh4wZvutRIo115RcqjO7MRR6eUBPMRADd8l8hFv5TXtEbkrdqPmNujAyikq46il3BcZKjxgt3DtaSRJXvWSnOh5BCpWgHJqCxE9eZnKBLSOKo-laMakLFbEyFsDAv7WK0oRg5vd5VfwDUsFt6evayYDV1b3HFUFvcrZQnx9d43ZEZBOeTc2Jjn_5-E02Kagbwp7leaim49na8NpPas1cG_wpmryqwODM89uMdhPuYOTx5Y90Nz2TOTyjl6uLlB1vRw3_-iLlx0pk8PvQNxxdz0UoMDF4BDAww.iw0bNTl_Ruiw1hLMta_SNg/__results___files/__results___40_1.png)

linkcode

**💭Thoughts:**

The model has identified false positives but never let even a single false negative which is more important than FP. Since we cant miss out a fraud transactions, but we can manage false positive results by investigating them

In \[69\]:

linkcode

```
#Classification metrics
print(classification_report(y_test, y_pred, target_names=['Not Fraud','Fraud']))

```

```
              precision    recall  f1-score   support

   Not Fraud       1.00      0.45      0.62     14902
       Fraud       0.65      1.00      0.79     15038

    accuracy                           0.73     29940
   macro avg       0.82      0.72      0.70     29940
weighted avg       0.82      0.73      0.70     29940

```

linkcode

**💭Thoughts:**

When we found that our false negatives are more important than false positives, we have to look at the recall number and we have 100% recall in finding the fraud transactions and 100% precision in finding the non fraud tranactions and on an average our model performs more than 70% accurate which is pretty good and there are possible chance to improve the performance of this model.

linkcode

# 🍃 Conclusion [¶](https://www.kaggle.com/code/benroshan/transaction-fraud-detection/notebook\#%F0%9F%8D%83-Conclusion)

![](https://www.comply-radar.com/wp-content/uploads/2019/09/ComplyRadar-august-08.png)

> With the advent of digital transactions, the possibility of money laundering have also soared up with the use of tech. Millions of investigators are on the field fighting against the fraudulent transactions. In the current industry we have a large inflow of false positives hits and it consumes a long time to clear the false positive hits. Customers across the world using fintech platforms demand lightning fast services. Hence automating the hits with machine learning and reducing the false positive hits is our aim. But not at the cost of leaving out the false negatives. Hence we need to be more mindful about false negatives when we try to reduce the false positives.
>
> Please share your comments on my work and do checkout my \[other notebooks\] [https://www.kaggle.com/benroshan/notebooks](https://www.kaggle.com/benroshan/notebooks))

## License

This Notebook has been released under the [Apache 2.0](http://www.apache.org/licenses/LICENSE-2.0) open source license.

## Continue exploring

- ![](https://www.kaggle.com/static/images/kernel/viewer/input_light.svg)







Input

1 file




arrow\_right\_alt

- ![](https://www.kaggle.com/static/images/kernel/viewer/output_light.svg)







Output

0 files




arrow\_right\_alt

- ![](https://www.kaggle.com/static/images/kernel/viewer/logs_light.svg)







Logs

4.9 second run - successful




arrow\_right\_alt

- ![](https://www.kaggle.com/static/images/kernel/viewer/comments_light.svg)







Comments

1 comment




arrow\_right\_alt